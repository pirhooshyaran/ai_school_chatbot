{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0e8ab0a",
   "metadata": {},
   "source": [
    "# RUN A SIMPLE LLM USING HUGGINGFACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f54ef43",
   "metadata": {},
   "source": [
    "### LOGIN TO HUGGINGFACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d53c078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged in to HuggingFace!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def huggingface_login():\n",
    "    \"\"\"\n",
    "    automates the login process to HuggingFace\n",
    "    \"\"\"\n",
    "\n",
    "    load_dotenv()\n",
    "    token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "    if not token:\n",
    "        raise ValueError(\"HF_TOKEN not found in environment variables or .env file\")\n",
    "    \n",
    "    try:\n",
    "        token_path = Path.home() / \".huggingface\" / \"token\"\n",
    "        token_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        token_path.write_text(token)\n",
    "\n",
    "        os.environ[\"HF_TOKEN\"] = token\n",
    "\n",
    "        subprocess.run([\"huggingface-cli\", \"login\", \"--token\", token], check=True)\n",
    "        subprocess.run([\"git\", \"config\", \"--global\", \"credential.helper\", \"store\"], check=True)\n",
    "        print(\"Successfully logged in to HuggingFace!\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        raise RuntimeError (f\"Failed to login to HuggingFace: {e}\")\n",
    "\n",
    "huggingface_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7c37ca",
   "metadata": {},
   "source": [
    "### Tokenizers in Large Language Models (LLMs)\n",
    "\n",
    "A **tokenizer** is the component of a large language model (LLM) that converts text into smaller pieces—called **tokens**—which the model can understand and process numerically.\n",
    "\n",
    "For example, take the sentence:  \n",
    "> “I love Machine Learning!”\n",
    "\n",
    "A tokenizer might split it into tokens like:  \n",
    "`[\"I\", \" love\", \" Machine\", \" Learning\", \"!\"]`\n",
    "\n",
    "Each token is then mapped to a unique number (an ID), such as:  \n",
    "`[100, 567, 8921, 2205, 33]`\n",
    "\n",
    "These IDs are what the LLM actually reads. Different tokenizers can split text differently—some by words, others by subwords or even characters—depending on how they were trained.\n",
    "\n",
    "The reverse process, **decoding**, converts token IDs back into readable text. For instance, decoding `[100, 567, 8921, 2205, 33]` would reconstruct the original:  \n",
    "> “I love Machine Learning!”\n",
    "\n",
    "In short, **tokenization** turns human language into numbers for the model, while **decoding** turns the model’s numeric outputs back into human language.\n",
    "\n",
    "\n",
    "### How Tokenizers Are Trained\n",
    "!!!!! Complete here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "943476de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User's Prompt:\n",
      "How you would explain machine learning in simple words?\n",
      "Bot:\n",
      "Imagine you have a bunch of data. Machine learning is like teaching a computer to learn from that data! \n",
      "\n",
      "Here's how it works:\n",
      "\n",
      "*   **Learning from data:** You feed the computer lots of examples.\n",
      "*   **Finding patterns:** The computer looks at the examples and tries to find patterns, like \"If you see this example, it's likely to be a spam email.\"\n",
      "*   **Making predictions:** Based on these patterns, the computer tries to make predictions about new, unseen data.\n",
      "*   **Giving it feedback:** You give the computer feedback, like \"That's not right.\"\n",
      "*   **Learning from feedback:** The computer adjusts its model based on the feedback, making it better at predicting the future.\n",
      "\n",
      "So, machine learning is like teaching a computer to learn and improve over time by analyzing data and making predictions.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# torch.backends.cudnn.enabled = False\n",
    "         \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\" # \"cpu\" or \"cuda\"\n",
    "\n",
    "# model_name = \"google/gemma-3-270m\"\n",
    "model_name = \"google/gemma-3-270m-it\"\n",
    "prompt = \"How you would explain machine learning in simple words?\"\n",
    "\n",
    "# load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    # torch_dtype controls precision for standard loading and tells PyTorch how to store and compute all model weights and activations\n",
    "    torch_dtype=torch.float32, # NOTE: only float32 and float64 work on CPU\n",
    "    device_map=device, # auto, cpu, cuda, cuda: 0 etc.\n",
    ")\n",
    "\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "formatted_prompt = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "\n",
    "print(f\"User's Prompt:\\n{prompt}\")\n",
    "print(\"Bot:\")\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "### EXTRACT RESPONSE\n",
    "# decode all tokens to text\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=False)\n",
    "\n",
    "# remove the input part (prompt) so only new tokens remain\n",
    "input_text = tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=False)\n",
    "response_text = generated_text[len(input_text):]\n",
    "\n",
    "# cleanup (remove special tags or whitespace)\n",
    "response_text = response_text.replace(\"<end_of_turn>\", \"\").strip()\n",
    "\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cbe2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simple_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
